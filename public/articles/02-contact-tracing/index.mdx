---
title: Building a Contact Tracing Platform
blurb:
  "Learn how to automate and build an application to trace exposures using
  Amazon's Cloud Development Kit (CDK)"
tags:
  - software
  - maths
author: kochie
jumbotron:
  src: omar-flores-MOO6k3RaiwE-unsplash.jpg
  alt: connections
publishedDate: 2021-05-05T09:00:00+10:00
---

> TLDR: I made a contact tracing website that you can access
> [here](https://ct.vercel.app)

### Table of Contents

# Introduction

Infrastructure as code is a a great way to manage the deployment and
maintainence of applications and the machines required to run. Writing
configurations for applications as code allows easy identitcal deployments and
testability. In this article I'm going to describe how to use the AWS Cloud
Development Kit (CDK) to create an application and deploy it to an AWS
environment.

AWS CDK is an open source framework to define cloud application resources. It's
avaliable in a number of languages including TypeScript, Golang, .Net, and Java,
and lets you develop the resources for your application in farmiliar languages
so your buisness doesn't need to learn a new one.

To show how easy it is to create an application with CDK I'g going to make
something topical. At the time of writing Melbourne is under it's 5th covid
related lockdown, one of the reasons given for the lockdown was that contact
tracing personnel were inundated with exposure sites which needed to be
contacted and their staff and visitors tested and their whereabouts traced. This
type of problem is well suited to an automated solution, so I figured, why don't
I try to build a contact tracing app?

<GithubProject repo="contact-tracing" owner="kochie" />

If you want to see any of the code I've written for this demo you can find it on
github.

# Architecture

## Backend

To start with, here is a basic architecture diagram of what were going to build.

![Backend Architecture Diagram](/articles/02-contact-tracing/ct-architecture.svg)

The backend architecture for this is quite simple, the system is based off of a
serverless architecture so its cost is proportional to the amount of use it
recieves. The API which will handle the contact tracing is managed through
Amazon AppSync a managed graphql service. Appsync is great because you can
update an API in realtime by just adding more data sources to fetch data for
queries, all you need to do is define a schema and where to fetch the data from
and you're mostly done. There are a few more steps if you want to define
authentication and more complicated data sources but for the post part it's plug
and play.

The checkin data for this applicaiton will be stored in Amazon DynamoDB. There
are a few reasons why I chose Dynamo, the first is that it's a fully managed
serverless product, so I don't have to worry about managing the service for the
most part. The second is that it's fast, I mean really fast, a standard query
takes about 5ms, that's pretty good (as we go on I'll talk about how I'm caching
a lot of the queries in Dynamo Dax but this didn't speed up most queries mor the
most part and I'll explain why later). The final reason is that the data can be
easily stored in dynamo as a key value pair. Each checkin looks something like
this example.

```json
{
  "location_id": "1",
  "user_id": "2",
  "checkin_datetime": "2021-06-01T00:00:00+10:00"
}
```

Super nice and simple, and it's really easy to search for a checkin based on the
time, user_id, and location_id. DynamoDB queries need a key to search upon and
can be refined using a "sort key", in the application I'm using the user_id and
location_id as "partition keys" while the date will be the sort key. To support
multiple pertition keys in Dynamo you need to have a GlobalSecondaryIndex.

Authentication is vital for real world services. Having the ability to verify
users who will access sensitive data (checkin data is Personal Identifying
Information [PII]) is absolutely needed. AppSync supports multiple simultaneous
authentication methods including API keys, IAM user signing, Amazon Cognito, and
external providers, for this demo I'm going to use Congnito. Amazon Cognito is a
managed authentication service which can store thousands of user credentials.

There are two components for the api in the `lib` directory, the `functions` and
the `schema`

### What is CDK?

The AWS CDK is a framework that can be used to define and organise resources on
the AWS cloud. Any project in CDK is setup using "stacks" which are individual
deployments that resources are grouped in, in actual fact CDK converts stacks
into Amazon CloudFormation templates and subsequently deploys the templates as
stacks in CloudFormation.

The CDK is avaliable in a bunch of different languages but for this demo I'm
going to use TypeScript to define the CDK stack.

> I'm using CDK v2 is this demo because it's pretty close to being released
> soon. CDK v2 has a bunch of improvements over the first version which you can
> read about on the
> [aws blog](https://docs.aws.amazon.com/cdk/latest/guide/work-with-cdk-v2.html)
> but for this demo it won't matter too much which version you use.

The first thing to do is install the CDK tool, now if you're using another
language like Go or Java the installation will be a bit different but there are
instructions for each language on the CDK website.

```bash
npm install -g aws-cdk@next
```

After this you can create a new CDK project based off the app template as shown
below.

```bash
cdk init app --language typescript
```

After this you should have a blank CDK project, woo! To verify you should have a
directory layout similar to the screenshot below.

```none
.
├── README.md
├── bin
│   └── myapp.ts
├── cdk.json
├── jest.config.js
├── lib
│   └── myapp-stack.ts
├── package-lock.json
├── package.json
├── test
│   └── myapp.test.ts
└── tsconfig.json
```

### Defining the Schema

GraphQL is first and foremost, a query language. It's used to describe the data
in your API, ask for specific data, and return predicatable results. All queries
to a graphql api are sent to one http endpoint which supports POST requests. The
data send in the post request defines the query/mutation and what information to
return.

For the graphql engine to understand what queries are valid it requires a
schema. A graphql schema is a great tool for defining the scope of the API, it
has lots of advantages over traditional http endpoint schemas. The first is that
it's self documenting; since the API is defined by the schema it's always
exactly what queries the graphql engine is expecting. Secondly, any graphql
inspector like [GraphiQL](https://www.electronjs.org/apps/graphiql) can
automatically read the schema and lint your requests as you write them (once you
set up authentication with the server). Lastly since graphql is a strictly typed
language all the requests defined in the schema have types that can be used,
expanded, and checked against meaning it's very hard to send an incorrect query.

For this project there are two query files, the first is the `scalars.graphql`
file. This is used for linting purposes during development, the graphql spec
defines a few specific data types that all engines must support (things like
`String` and `Float`) but any engine implementation can have extra built-in
types, in fact you can specify your own in the schema. AppSync has several built
in types that we're going to use so to help tools like vscode and intellij
understand these different types we can define another schema file that is in
the same directory as our main schema that won't get used by the CDK stack but
will be used by any tools that scan the directory.

For a complete list of AppSync built-in scalar types you can check out the
[documentation](https://docs.aws.amazon.com/appsync/latest/devguide/scalars.html).

```graphql
# scalars.graphql

scalar AWSTimestamp
scalar AWSURL
scalar AWSDate
scalar AWSDateTime

directive @aws_api_key on FIELD_DEFINITION | INPUT_OBJECT | OBJECT | ENUM
directive @aws_cognito_user_pools on FIELD_DEFINITION | INPUT_OBJECT | OBJECT | ENUM_VALUE
```

```graphql
# schema.graphql

type CheckIn {
  location_id: String
  user_id: String
  checkin_datetime: AWSDateTime
}

type Output {
  items: [CheckIn]
  nextToken: String
}

type Exposures {
  users: [String]
  locations: [String]
}

type User {
  user_id: String
  contacts: [Location]
}

type Location {
  time: AWSDateTime
  location_id: String
  visitors: [User]
}

type LocationFlat {
  location_id: String
  latitude: String
  longitude: String
}

type Node {
  user_id: String
}

type Link {
  source: String
  target: String
  time: String
  location_id: String
  latitude: String
  longitude: String
}

type Flat {
  nodes: [Node]
  links: [Link]
  locations: [LocationFlat]
}

type Query {
  get_user_location_history(
    user_id: String!
    from: AWSDateTime
    until: AWSDateTime
    nextToken: String
    limit: Int
  ): Output
  get_location_attendees(
    location_id: String!
    from: AWSDateTime
    until: AWSDateTime
    nextToken: String
    limit: Int
  ): Output
  trace_exposure(
    user_id: String!
    from: AWSDateTime
    until: AWSDateTime
  ): Exposures
  trace_exposure_tree(
    user_id: String!
    from: AWSDateTime
    until: AWSDateTime
  ): User
  trace_exposure_flat(
    user_id: String!
    from: AWSDateTime
    until: AWSDateTime
  ): Flat
  trace_exposure_over_time(
    user_id: String!
    from: AWSDateTime
    until: AWSDateTime
    step: Int
    depth: Int
  ): [Flat]
}

type Mutation {
  check_in(location_id: String!, user_id: String!): CheckIn
}

type Schema {
  query: Query
  mutation: Mutation
}
```

### Writing the Functions

The backbone of an AppSync API are the data sources. The data sources map to a
query or mutation object and tell AppSync where to get or set the data for the
operation. At the time of writing there are 6 different data sources: DynamoDB,
Elasticsearch, Lambda, RDS, HTTP, and None (this is just an empty placeholder).

This API uses two different data sources. For simple key retrivals the API has a
DynamoDB resolver, this data source can query the table directly using a mapping
function and return the results. There are a few advantages to doing this, the
main one is that you don't need to write an extra lambda function in another
language to query the database.

The other resolver used is the Lambda provider. AppSync can use Amazon Lambda
Functions as a data source provider, using lambda to run functions allows your
api to run any arbitary code needed to get a result for the api. Since lambda
functions can now run docker images there isn't really any function that your
api can't do.

#### A quick note on CDK v2 and Golang Lambda Functions

At the time of writing this article one of the major flaws in CDK v2 is that
there is no support for using languages other than JavaScript to write lambda
functions with and have them automatically bundled and deployed with the stack.
For the first version of CDK [Rafal Wilinski](https://github.com/RafalWilinski)
wrote a
[custom construct](https://github.com/RafalWilinski/aws-lambda-golang-cdk) that
will compile Golang lambda functions and bundle them with the stack however,
this construct doesn't work with v2 of the CDK. So to fix this I spent the
better part of a weekend a few weeks ago patching the code Rafal wrote and
making a version that is compatible with v2 of the CDK which you can find
[here](https://github.com/kochie/aws-lambda-golang-cdk-v2).

#### Common

```go
// utilities.go
// NOTE: This is just an snippet of the entire file, to see the entire source goto
// https://github.com/kochie/contact-tracing/blob/master/lib/functions/common/utilities.go
package common

import (
	"context"
	"log"
	"os"
	"time"

	"github.com/aws/aws-dax-go/dax"
	"github.com/aws/aws-sdk-go-v2/aws"
	"github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/feature/dynamodb/attributevalue"
	"github.com/aws/aws-sdk-go-v2/service/dynamodb"
	"github.com/aws/aws-sdk-go-v2/service/dynamodb/types"
	"github.com/aws/aws-xray-sdk-go/xray"
)

var client *dax.Dax

var tableName = os.Getenv("TABLE_NAME")

type CheckIn struct {
	UserID          string    `dynamodbav:"user_id"`
	LocationID      string    `dynamodbav:"location_id"`
	CheckinDatetime time.Time `dynamodbav:"checkin_datetime"`
	Latitude        string    `dynamodbav:"latitude"`
	Longitude       string    `dynamodbav:"longitude"`
}

func init() {
	daxEndpoint := os.Getenv("DAX_ENDPOINT")

	cfg, err := config.LoadDefaultConfig(context.TODO())
	if err != nil {
		log.Fatal(err)
	}

	err = xray.Configure(xray.Config{
		ServiceVersion: "1.2.3",
	})

	if err != nil {
		log.Fatal(err)
	}

	xray.AppendMiddlewares(&cfg.APIOptions)
	log.Println("Xray middleware applied")

	daxCfg := dax.DefaultConfig()
	daxCfg.HostPorts = []string{daxEndpoint}
	daxCfg.Region = "ap-southeast-2"
	daxCfg.RequestTimeout = 5 * time.Minute
	client, err = dax.New(daxCfg)
	if err != nil {
		log.Fatal(err)
	}
	log.Println("Dynamo Accelerator configured")
	log.Println("Ready to work.")
}

func GetLocationVisitors(locationId, from, until string, ctx context.Context) ([]*CheckIn, error) {
	var expression string
	expressionAttributeValues := map[string]types.AttributeValue{
		":location_id": &types.AttributeValueMemberS{Value: locationId},
	}

	if from != "" && until != "" {
		expression = `location_id = :location_id AND checkin_datetime BETWEEN :from AND :until`
		expressionAttributeValues[":from"] = &types.AttributeValueMemberS{Value: from}
		expressionAttributeValues[":until"] = &types.AttributeValueMemberS{Value: until}
	} else if until != "" {
		expression = `location_id = :location_id AND checkin_datetime <= :until`
		expressionAttributeValues[":until"] = &types.AttributeValueMemberS{Value: until}
	} else if from != "" {
		expression = `location_id = :location_id AND checkin_datetime >= :from`
		expressionAttributeValues[":from"] = &types.AttributeValueMemberS{Value: from}
	} else {
		expression = `location_id = :location_id`
	}

	paginator := dynamodb.NewQueryPaginator(client, &dynamodb.QueryInput{
		TableName:                 aws.String(tableName),
		KeyConditionExpression:    aws.String(expression),
		ExpressionAttributeValues: expressionAttributeValues,
	})

	locations := make([]*CheckIn, 0)
	for paginator.HasMorePages() {
		resp, err := paginator.NextPage(ctx)

		if err != nil {
			return nil, err
		}

		for _, item := range resp.Items {
			checkin := CheckIn{}
			err := attributevalue.UnmarshalMap(item, &checkin)
			if err != nil {
				return nil, err
			}

			locations = append(locations, &checkin)
		}
	}

	return locations, nil
}
```

```go
package main

import (
	"context"
	"time"

	"github.com/aws/aws-lambda-go/lambda"
	"github.com/kochie/contact-tracing/lib/functions/common"
)

type User struct {
	UserId   string      `json:"user_id"`
	From     string      `json:"-"`
	Contacts []*Location `json:"contacts"`
}

type Location struct {
	Time       time.Time `json:"time"`
	LocationId string    `json:"location_id"`
	Visitors   []*User   `json:"visitors"`
}

func HandleRequest(ctx context.Context, event interface{}) (*User, error) {
	eventData := event.(map[string]interface{})
	arguments := eventData["arguments"].(map[string]interface{})
	userId := arguments["user_id"].(string)
	from := ""
	if _, ok := arguments["from"].(string); ok {
		from = arguments["from"].(string)
	}
	until := ""
	if _, ok := arguments["until"].(string); ok {
		until = arguments["until"].(string)
	}

	seenUsers := make(map[string]bool)
	seenLocations := make(map[string]bool)

	rootUser := User{UserId: userId, From: from, Contacts: make([]*Location, 0)}
	stack := []*User{&rootUser}

	for len(stack) > 0 {
		user := stack[0]
		from = user.From
		stack = stack[1:]

		if _, ok := seenUsers[user.UserId]; ok {
			continue
		}

		seenUsers[user.UserId] = true
		checkins, err := common.GetUserLocationHistory(user.UserId, from, until, ctx)
		if err != nil {
			return nil, err
		}
		for _, checkin := range checkins {
			locationID := checkin.LocationID
			if _, ok := seenLocations[locationID]; ok {
				continue
			}
			seenLocations[locationID] = true

			f := checkin.CheckinDatetime.Add(-time.Hour).Format(time.RFC3339)
			u := checkin.CheckinDatetime.Add(time.Hour).Format(time.RFC3339)
			visitors, err := common.GetLocationVisitors(locationID, f, u, ctx)
			if err != nil {
				return nil, err
			}

			users := make([]*User, 0)
			for _, visitor := range visitors {
				if _, ok := seenUsers[visitor.UserID]; ok {
					continue
				}

				u := User{visitor.UserID, checkin.CheckinDatetime.Format(time.RFC3339), make([]*Location, 0)}
				stack = append(stack, &u)
				users = append(users, &u)
			}

			user.Contacts = append(user.Contacts, &Location{
				checkin.CheckinDatetime,
				locationID,
				users,
			})
		}
	}

	return &rootUser, nil
}

func main() {
	lambda.Start(HandleRequest)
}
```

### Building the Stack

Inside of the `lib` directory there should be a file similar to
`contact-tracing-stack.ts` this file contains the stack and resources that will
be added to the app. Inside the file there should be a single class which will
be the stack definiton.

```typescript
export class ContactTracingStack extends Stack {
  constructor(scope: Construct, id: string, props?: StackProps) {
    super(scope, id, props)

    // Define resources here
  }
}
```

From here all the resources will be defined within the scope of the class
constructor [^2].

#### VPC

```typescript
const vpc = new ec2.Vpc(this, 'contacts-vpc', {
  cidr: Vpc.DEFAULT_CIDR_RANGE,
  subnetConfiguration: [
    {
      cidrMask: 24,
      name: 'default',
      subnetType: ec2.SubnetType.PUBLIC,
    },
    {
      cidrMask: 24,
      name: 'contact-tracing',
      subnetType: ec2.SubnetType.PRIVATE,
    },
  ],
})
```

#### DynamoDB Table

There are two resources that need to be created for the Dynamo table. The first
is the table itself, and the second is a global secondary index. The Dynamo
table will allow querying using a single partition key and optional hash key for
this table the location id will be the partition key and the check in date will
be the hash key. This will allow queries to easily query the locations but won't
allow queries related to the user id. The way to solve this using Dynamo is to
create another index. There are two types of secondary indexes in Dynamo, Global
Secondary Indexes allow for another partition key while Local Secondary Indexes
provide another sort key.

```typescript
const contact_table = new dynamo.Table(this, 'contact-tracing-table', {
  tableName: 'checkins',
  encryption: dynamo.TableEncryption.AWS_MANAGED,
  partitionKey: {
    name: 'location_id',
    type: dynamo.AttributeType.STRING,
  },
  sortKey: {
    name: 'checkin_datetime',
    type: dynamo.AttributeType.STRING,
  },
  billingMode: BillingMode.PAY_PER_REQUEST,
})

contact_table.addGlobalSecondaryIndex({
  indexName: 'index_by_user',
  partitionKey: {
    name: 'user_id',
    type: dynamo.AttributeType.STRING,
  },
  sortKey: {
    name: 'checkin_datetime',
    type: dynamo.AttributeType.STRING,
  },
})
```

#### Using Dynamo Dax

Over the course of this project I found that some of the larger requests were
taking up to 16 seconds to run. In terms of making a responsive website that's
incredibly slow. As I investigated the issue I found that over 99% of the
execution time was being used witing for DynamoDB queries. Now like I said at
the top of this article Dynamo is really fast, queries usually take less than
5ms to run, but when you're creating a contact tracer that has to cross
reference thousands, potentially millions of people and locations those 5ms add
up. To speed up these request I decided to use DynamoDB Accellerator (DAX) to
cache the results of any Dynamo queries. My intial thought was that most of the
queries that would be executed would be repeats of previous queries, this
assumption was wrong.

**Rant Alert**

So as I'll mention later in the article I'm using Golang to write the lambda
functions, to use the AWS API in Golang you can use the offical SDKs. The latest
version of the SDK `aws-sdk-v2` has been in active public development for a few
years and was classified generally avaliable in January of tjos year. Now the
new SDK has a bunch of useful features that make it much easier to use which
I'll go into later, but for the time being I wrote most of the function code
before adding support for DAX. Now the new SDK does not have out of the box
support for DAX, due to the architecture of how DAX works it would require a
major rewrite[^3]. This is the same as the original SDK, it doesn't have support
either, but there is [another SDK](https://github.com/aws/aws-dax-go) built for
DAX that has support for v1 of the Golang SDK. Now here in lies the problem,
because v2 of teh Golang SDK is a complete rewrite, this library is incompatible
with the new SDK version. Now you;d think that there would be support for the
new SDk considering both of these repos are official AWS repositories but alas.
So I took it upon myself to try to add support for the new version which you can
see [here](https://github.com/kochie/aws-dax-go). So great sounds like problem
solved right? Well no because the DAX protocol is pretty confusing, to get the
speed they're promising the DAX team isn't using http or TCP to talk to the DAX
server. So all I've done is change the input and output structs of the SDK to
support v2. Is this an eligant solution, no. Is this a good solution, also no,
but it is a solution.

```go.mod
replace (
	github.com/aws/aws-dax-go => github.com/kochie/aws-dax-go master
)
```

After all this work I tested the API calls and success! Kind of, well the first
request is still slow but any identical subsequent requests are much faster. The
only problem with this is that most of the requests are not going to be
identical so the speedup isn't really worth the effort. I've documented what
I've done for the sake of completeness but I don't recommend using DAX for a
problem like this.

```typescript
const daxRole = new iam.Role(this, 'dax-role', {
  assumedBy: new iam.ServicePrincipal('dax.amazonaws.com'),
  inlinePolicies: {
    dynamo_access: new iam.PolicyDocument({
      statements: [
        new iam.PolicyStatement({
          actions: ['dynamodb:*'],
          effect: iam.Effect.ALLOW,
          resources: [
            contact_table.tableArn,
            `${contact_table.tableArn}/index/index_by_user`,
          ],
        }),
      ],
    }),
  },
})

const securityGroup = new ec2.SecurityGroup(this, 'security-group-vpc', {
  vpc,
})
securityGroup.addIngressRule(ec2.Peer.anyIpv4(), ec2.Port.tcp(8111), 'DAX')

const daxSubnetGroup = new dax.CfnSubnetGroup(this, 'dax-subnet-group', {
  subnetGroupName: 'contact-tracing',
  subnetIds: vpc.privateSubnets.map((subnet) => subnet.subnetId),
})

const cache = new dax.CfnCluster(this, 'dax-contacts', {
  clusterName: 'contacts',
  description: 'DAX cache for contact tracing',
  iamRoleArn: daxRole.roleArn,
  nodeType: 'dax.t3.small',
  replicationFactor: 1,
  securityGroupIds: [securityGroup.securityGroupId],
  sseSpecification: {
    sseEnabled: true,
  },
  subnetGroupName: daxSubnetGroup.subnetGroupName,
})
cache.addDependsOn(daxSubnetGroup)
```

#### Cognito User Pool and Client

Next to set up Cognito there are two resources that need to be created. The
first is the Cognito User Pool, this resouce will hold all the information about
users who aceess the API, mainly their email and password. The second component
is the Cognito App Client, this resource is used as a endpoint for web services
to talk to Cognito, it specifies what authentication methods valid.

```typescript
const user_pool = new cognito.UserPool(this, 'ct-user-pool', {
  userPoolName: 'contact-tracing-pool',
  signInCaseSensitive: false,
  selfSignUpEnabled: true,
  signInAliases: {
    email: true,
  },
  autoVerify: {
    email: true,
  },
  standardAttributes: {
    email: { required: true },
  },
  passwordPolicy: {
    requireSymbols: false,
    requireDigits: false,
    requireLowercase: false,
    requireUppercase: false,
  },
})

const user_pool_client = new cognito.UserPoolClient(
  this,
  'ct-user-pool-client',
  {
    userPool: user_pool,
    userPoolClientName: 'contact-tracing-client',
  }
)
```

#### AppSync API Schema

Now with the dependencies out of the way we can now create the AppSync API. The
first part is to define a new AppSync API resource and the schema the resource
will use.

```typescript
const api = new appsync.CfnGraphQLApi(this, 'ct-api', {
  authenticationType: 'AMAZON_COGNITO_USER_POOLS',
  name: 'contact-tracing-api',
  userPoolConfig: {
    userPoolId: user_pool.userPoolId,
    awsRegion: this.region,
    defaultAction: 'ALLOW',
  },
  logConfig: {
    cloudWatchLogsRoleArn:
      'arn:aws:iam::457234467265:role/service-role/appsync-graphqlapi-logs-ap-southeast-2',
    excludeVerboseContent: false,
    fieldLogLevel: 'ALL',
  },
})

const api_schema = new appsync.CfnGraphQLSchema(this, 'ct-api-schema', {
  apiId: api.attrApiId,
  definition: readFileSync(
    join(__dirname, 'graphql/schema.graphql')
  ).toString(),
})
```

This will create an AppSync API that will use the attached schema. But at the
moment none of the data values in the schema are connected to a resolver. To do
this we'll need to create some data sources and resolvers.

#### AppSync Data Sources and Resolvers

```typescript
const getUserLocationHistoryResolver = new appsync.CfnResolver(
  this,
  'getUserLocationHistoryQueryResolver',
  {
    apiId: api.attrApiId,
    typeName: 'Query',
    fieldName: 'get_user_location_history',
    dataSourceName: dataSource.name,
    requestMappingTemplate: `{
      "version": "2018-05-29",
      "operation":  "Query",
      "query": {
        "expression": "user_id = :userId \
        #if( $ctx.args.from && $ctx.args.until )
          AND checkin_datetime BETWEEN :from AND :until",
        #elseif( $ctx.args.from )
          AND checkin_datetime >= :from",
        #elseif( $ctx.args.until )
          AND checkin_datetime <= :until",
        #else
          ",
        #end
        "expressionValues": {
          ":userId": $util.dynamodb.toDynamoDBJson($ctx.args.user_id),
        #if( $ctx.args.from ) 
          ":from": $util.dynamodb.toDynamoDBJson($ctx.args.from),
        #end
        #if( $ctx.args.until ) 
          ":until": $util.dynamodb.toDynamoDBJson($ctx.args.until),
        #end
        }
      },
      "index": "index_by_user",
      "limit": $util.defaultIfNull($ctx.args.limit, 20),
      "nextToken": $util.toJson($util.defaultIfNullOrBlank($ctx.args.nextToken, null))
    }`,
    responseMappingTemplate: `{
      "items": $util.toJson($ctx.result.items),
      "nextToken": $util.toJson($util.defaultIfNullOrBlank($ctx.result.nextToken, null))
    }`,
  }
)
getUserLocationHistoryResolver.addDependsOn(api_schema)
```

```typescript
const table_role = new iam.Role(this, 'ItemsDynamoDBRole', {
  assumedBy: new iam.ServicePrincipal('appsync.amazonaws.com'),
})
table_role.addManagedPolicy(
  iam.ManagedPolicy.fromAwsManagedPolicyName('AmazonDynamoDBFullAccess')
)

const dataSource = new appsync.CfnDataSource(this, 'ItemsDataSource', {
  apiId: api.attrApiId,
  name: 'ItemsDynamoDataSource',
  type: 'AMAZON_DYNAMODB',
  dynamoDbConfig: {
    tableName: contact_table.tableName,
    awsRegion: this.region,
  },
  serviceRoleArn: table_role.roleArn,
})
```

```typescript
const dataSource = new appsync.CfnDataSource(this, 'ItemsDataSource', {
  apiId: api.attrApiId,
  name: 'ItemsDynamoDataSource',
  type: 'AMAZON_DYNAMODB',
  dynamoDbConfig: {
    tableName: contact_table.tableName,
    awsRegion: this.region,
  },
  serviceRoleArn: table_role.roleArn,
})
```

```typescript
const checkinResolver = new appsync.CfnResolver(
  this,
  'checkinMutationResolver',
  {
    apiId: api.attrApiId,
    typeName: 'Mutation',
    fieldName: 'check_in',
    dataSourceName: dataSource.name,
    requestMappingTemplate: `{
      "version": "2018-05-29",
      "operation": "PutItem",
      "key": {
        "location_id": $util.dynamodb.toDynamoDBJson($ctx.args.location_id),
        "checkin_datetime": $util.dynamodb.toDynamoDBJson($util.time.nowISO8601())
      },
      "attributeValues": {
        "user_id": $util.dynamodb.toDynamoDBJson($ctx.args.user_id)
      }
    }`,
    responseMappingTemplate: `$util.toJson($ctx.result)`,
  }
)
checkinResolver.addDependsOn(api_schema)
```

## Frontend

The frontend of the application is a react site using
[next.js](https://nextjs.com). Next is a great framework for developing websites
with react and can be hosted on any static web service. For this application I'm
using [vercel.com](https://vercel.com) as it's a free service that works really
well with next since they're both made by the same team. I do want to point out
that Amazon Amplify has great support for nextjs as well but I'm using vercel
for legacy reasons.

The first step is to create a next app, this can be done with the
`create-next-app` tool which is pretty similar to `create-react-app` if you're
farmiliar with that.

```bash
npx create-next-app frontend/
```

After running the setup command you should have a directory something like this.

```none
frontend
├── README.md
├── package-lock.json
├── package.json
├── pages
│   ├── _app.js
│   ├── api
│   │   └── hello.js
│   └── index.js
├── public
│   ├── favicon.ico
│   └── vercel.svg
└── styles
    ├── Home.module.css
    └── globals.css
```

We now have a boilerplate nextjs project, running `npm run dev` will start up
the development server and you should see a starting page. We're not going to be
using this page or the api setup by next so you can delete the `api/` directory
and `index.js`.

### Setting up Authentication

When we set up the backend we created a Cognito userpool and client, we're going
to use those resources to get an authentication token that will work with
AppSync, the flow should work something like this:

1. User loads the website.
2. The website checks to see if the user is logged in.
3. If yes go to step 5.
4. Direct user to login or signup.
5. When user logged in ask Cognito for an authorization token to send with API
   requests.
6. Cognito returns a token that can be used.

These steps are pretty straight forward but can be very tricky to do correctly.
Fortunately there is a pre build library for Cognito authentication that we can
use.

```bash
npm install --save-dev aws-amplify
```

Once the package is installed we can configure the `Auth` object to connect to
our Cognito client, for this we need the ID of the user pool and the client
which we had exported from our stack before. Let's save them for the time being
and add them to the code as environment variables.

```typescript{9-13}
import React, { useEffect } from "react";
import { Auth } from "aws-amplify";
import { ApolloProvider } from "@apollo/client";
import { useApollo } from "../lib/apolloClient";

import type { AppProps } from 'next/app'
import "tailwindcss/tailwind.css";

Auth.configure({
  userPoolId: process.env.NEXT_PUBLIC_USERPOOL_ID,
  userPoolWebClientId: process.env.NEXT_PUBLIC_CLIENT_ID,
  region: process.env.NEXT_PUBLIC_REGION,
});

function App({ Component, pageProps }: AppProps) {
  const apolloClient = useApollo(pageProps.initialApolloState);

  return (
    <ApolloProvider client={apolloClient}>
      <Component {...pageProps} />
    </ApolloProvider>
  );
}

export default App;
```

### Setting up Apollo

Apollo is a GraphQL implementation that can be used to generate and create an
API similarly to AppSync. For our uses however, we're going to be using the open
source client library that Apollo provides. First thing to do is install Apollo.

```bash
npm install --save-dev @apollo/client
```

After Apollo is installed we need to configure it to work with a nextjs react
app and the authentication method we've created. There is an example in the
[nextjs repo](https://github.com/vercel/next.js/tree/canary/examples/with-apollo)
that explains how to connect Apollo. To connect with the Amplify auth library
all we need to do is fetch the token which can be seen in the `authLink`
context.

```typescript{23-24,31}
// frontend/src/lib/apolloClient

import { useMemo } from 'react'
import {
  ApolloClient,
  createHttpLink,
  from,
  InMemoryCache,
} from '@apollo/client'
import { Auth } from 'aws-amplify'
import { setContext } from '@apollo/client/link/context'
import { onError } from '@apollo/client/link/error'
import { Output } from '../queries/common'

let apolloClient
const httpLink = createHttpLink({
  uri: process.env.NEXT_PUBLIC_API_URL,
})

const authLink = setContext(async (request, { headers }) => {
  // get the authentication token from local storage if it exists
  let token
  try {
    const session = await Auth.currentSession()
    token = session.getAccessToken().getJwtToken()
  } catch {
    console.log('NO TOKEN!!!!')
  }
  return {
    headers: {
      ...headers,
      Authorization: token ? token : '',
    },
  }
})

function createApolloClient() {
  return new ApolloClient({
    ssrMode: typeof window === 'undefined',
    link: from([
      onError((err) => {
        console.log(err)
      }),
      authLink,
      httpLink,
    ]),
    cache,

    name: 'react-web-client',
    version: '1.3',
    queryDeduplication: false,
    defaultOptions: {
      watchQuery: {
        fetchPolicy: 'cache-first',
      },
    },
  })
}

export function initializeApollo(initialState = null) {
  const _apolloClient = apolloClient ?? createApolloClient()

  // If your page has Next.js data fetching methods that use Apollo Client, the initial state
  // gets hydrated here
  if (initialState) {
    // Get existing cache, loaded during client side data fetching
    const existingCache = _apolloClient.extract()
    // Restore the cache using the data passed from getStaticProps/getServerSideProps
    // combined with the existing cached data
    _apolloClient.cache.restore({ ...existingCache, ...initialState })
  }
  // For SSG and SSR always create a new Apollo Client
  if (typeof window === 'undefined') return _apolloClient
  // Create the Apollo Client once in the client
  if (!apolloClient) apolloClient = _apolloClient

  return _apolloClient
}

export function useApollo(initialState) {
  return useMemo(() => initializeApollo(initialState), [initialState])
}
```

The `apolloClient` file creates a react hook that can be used by next as a
provider - a type of react component that _provides_ a context. A Context
provides a way to pass data through the component tree without having to pass
props down manually at every level [^1].

```typescript{15,18-20}
import React, { useEffect } from "react";
import type { AppProps } from 'next/app'
import { Auth } from "aws-amplify";
import { ApolloProvider } from "@apollo/client";
import { useApollo } from "../lib/apolloClient";
import "tailwindcss/tailwind.css";

Auth.configure({
  userPoolId: process.env.NEXT_PUBLIC_USERPOOL_ID,
  userPoolWebClientId: process.env.NEXT_PUBLIC_CLIENT_ID,
  region: process.env.NEXT_PUBLIC_REGION,
});

function App({ Component, pageProps }: AppProps) {
  const apolloClient = useApollo(pageProps.initialApolloState);

  return (
    <ApolloProvider client={apolloClient}>
      <Component {...pageProps} />
    </ApolloProvider>
  );
}

export default App;
```

### Building a D3 Component

![](/articles/02-contact-tracing/tree.svg)

# Final Thoughts

This project was actually a lot more complicated then I first assumed it would
be, the fact that both Dynamo Dax and XRay didn't have support for Golangs
`aws-sdk-v2` really slowed down my development. The D3 charts also took a long
time to make, but once they started to work it was okay to iterate and improve.

[^2]: So there are multiple imports
[^1]:
    The definition provided by the
    [react docs](https://reactjs.org/docs/context.html).
